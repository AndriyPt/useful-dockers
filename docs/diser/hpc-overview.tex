\section{Архітектура багатопроцесорних обчислювальних систем}

У процесі розвитку суперкомп'ютерних технологій ідею підвищення продуктивності обчислювальної системи за рахунок збільшення кількості процесорів використовували неодноразово. 
% Якщо не вдаватися до історичного екскурсу і обговорення всіх таких спроб, то можна коротко описати розвиток подій.

Експериментальні розробки зі створення багатопроцесорних обчислювальних систем почалися в 70-х роках XX століття. Однією з перших таких систем стала розроблена в університеті Ілінойса багатопроцесорна обчислювальна система ILLIAC IV~\cite{illiac4}, яка включала 64 (у проекті до 256) процесорних елемента (ПЕ), що працюють за єдиною програмою, яка застосовується до вмісту власної оперативної пам'яті кожного ПЕ. 

Обмін даними між процесорами здійснювався через спеціальну матрицю комунікаційних каналів. Вказана особливість комунікаційної системи дала назву ''Матричні суперкомп'ютери'' відповідному класу мультипроцесорних обчислювальних систем (МОС). Ширший клас МОС з розподіленою пам'яттю і з довільною комунікаційною системою отримав надалі назву “Багатопроцесорні системи з масовим паралелізмом”, або МОС з MPP-архітектурою. При цьому, як правило, кожен з ПЕ MPP системи є універсальним процесором, що діє за своєю власною програмою (на відміну від спільної програми для всіх ПЕ матричної МОС).

Перші матричні МОС випускалися буквально поштучно, тому їх вартість була надзвичайно високою. Серійні ж зразки подібних систем, такі як ICL DAP, що включали до 8192 ПЕ, з'явилися значно пізніше, проте не набули широкого поширення, зважаючи на складність програмування МОС з одним потоком управління (з однією програмою, спільною для всіх ПЕ). Перші промислові зразки мультипроцесорних систем з'явилися на базі век\-тор\-но-кон\-ве\-єр\-них комп'ютерів у середині 80-х років XX ст. 

Найбільш поширеними МОС такого типу були суперкомп'ютери фірми Cray. Проте такі системи були надзвичайно дорогими і виготовлялися невеликими серіями. Як правило, в подібних комп'ютерах об'єднувалося від 2 до 16 процесорів, які мали рівноправний (симетричний) доступ до спільної оперативної пам'яті. У зв'язку з цим вони отримали назву симетричні мультипроцесорні системи (Symmetric Multi-Processing – SMP).

Як альтернатива таким дорогим мультипроцесорним системам на базі век\-тор\-но-кон\-ве\-єр\-них процесорів була запропонована ідея будувати еквівалентні за потужністю багатопроцесорні системи з великої кількості дешевих мікропроцесорів, що серійно випускалися.

Проте дуже скоро виявилося, що SMP архітектура має досить обмежені можливості з нарощування кількості процесорів в системі через різке збільшення числа конфліктів при зверненні до спільної шини пам'яті.   
У зв'язку з цим виправданою представлялася ідея забезпечити кожен процесор власною оперативною пам'яттю, перетворюючи комп'ютер на об'єднання незалежних обчислювальних вузлів.   

Такий підхід значно збільшив міру масштабованості багатопроцесорних систем, але у свою чергу почав вимагати розробки спеціального способу обміну даними між обчислювальними вузлами, що реалізовується зазвичай у вигляді механізму передачі повідомлень (Message Passing).

Комп'ютери з такою архітектурою є найбільш яскравими представниками MPP систем. У даний час ці два напрямки (або деякі їх комбінації) є домінуючими в розвитку суперкомп'ютерних технологій.  

Чимось середнім між SMP і MPP є NUMA-архітектури, в яких пам'ять фізично розділена, але логічно загальнодоступна. При цьому час доступу до різних блоків пам'яті стає неоднаковим. У одній з перших систем цього типу Cray T3D час доступу до пам'яті іншого процесора був у 6 разів більше, ніж до своєї власної.

На даний час розвиток суперкомп'ютерних технологій йде чотирма основними напрямками: 

\begin{itemize}
% \item SMP системи; 
\item MPP системи;
\item кластерні системи;
\item архітектура типу сузір'я. 
\end{itemize}

Розглянемо основні особливості перерахованих архітектур.

% \subsection{Векторно-конвеєрні суперкомп'ютери}
% 
% Перший векторно-конвеєрний комп'ютер Cray-1 з'явився в 1976 році. Його архітектура виявилася настільки вдалою, що він поклав початок цілому сімейству комп'ютерів. Назву цьому сімейству комп'ютерів дали два принципи, закладені в архітектурі процесорів:
% 
% \begin{itemize}
% \item конвеєрна організація обробки потоку команд;
% \item введення в систему команд набору векторних операцій, які дозволяють оперувати з цілими масивами даних.   
% \end{itemize}
% 
% Довжина векторів, що обробляються одночасно, в сучасних векторних комп'ютерах складає 128 або 256 елементів. Очевидно, що векторні процесори повинні мати набагато складнішу структуру та містити безліч арифметичних пристроїв. 
% 
% Основне призначення векторних операцій полягає в розпаралелюванні виконання операторів циклу, в яких в основному і зосереджена велика частина обчислювальної роботи. Для цього цикли піддаються процедурі векторизації з тим, щоб вони могли реалізовуватися з використанням векторних команд. Як правило, це виконується автоматично компіляторами під час генерації ними коду програми. Тому векторно-конвеєрні комп'ютери не вимагали якоїсь спеціальної технології програмування, що і виявилось вирішальним чинником їх успіху на комп'ютерному ринку. Проте, було потрібне дотримання деяких правил при написанні циклів з тим, щоб компілятор міг ефективно їх векторизувати.
% 
% Історично це були перші комп'ютери, до яких повною мірою можна було застосувати поняття ``суперкомп'ютер''. Декілька векторно-конвеєрних процесорів (2-16) працюють в режимі зі спільною пам'яттю (SMP), утворюючи обчислювальний вузол, а декілька таких вузлів об'єднуються за допомогою комутаторів, утворюючи або NUMA, або MPP систему. 
% 
% Типовими представниками такої архітектури є комп'ютери CRAY J90/T90, CRAY SV1, NEC SX-4/SX-5.
% 
% Рівень розвитку мікроелектронних технологій того часу не дозволяв виробляти однокристальні векторні процесори, тому ці системи буди досить громіздкі та надзвичайно дорогі. 
% 
% У зв'язку з цим, починаючи з середини 90-х років, коли з'явилися досить потужні суперскалярні мікропроцесори, інтерес до цього напрямку був в значній мірі ослаблений. 
% 
% Суперкомп'ютери з векторно-конвеєрною архітектурою стали програвати системам з масовим паралелізмом. 
% Проте в березні 2002 р. корпорація NEC представила систему Earth Simulator з 5120 векторно-конвеєрними процесорами, яка в 5 разів перевищила продуктивність попереднього володаря рекорду – MPP системи ASCI White з 8192 суперскалярними мікропроцесорами. Це, звичайно ж, змусило багатьох по-новому поглянути на перспективи векторно-конвеєрних систем.
% 
% На сьогоднішній день векторні операції підтримують всі сучасні процесори загального вжитку, і векторно-конвеєрна архітектура розвивається як їх невід'єма частина.

% \subsection{Симетричні мультипроцесорні системи (SMP)}
% 
% Характерною рисою багатопроцесорних систем SMP архітектури є те, що всі процесори мають прямий і рівноправний доступ до будь-якої адреси спільної пам'яті.  
% 
% Перші системи SMP складалися з кількох однорідних процесорів та масиву спільної пам'яті, до якої процесори підключалися через спільну системну шину. Однак дуже скоро виявилося, що така архітектура непридатна для створення масштабних систем.
% 
% Основний недолік – велика кількість конфліктів при зверненні до спільної шини. Гостроту цієї проблеми вдалося частково зняти розділенням пам'яті на блоки, підключення до яких за допомогою комутаторів дозволило розпаралелювати звернення від різних процесорів. Та і при такому підході для систем більш ніж з 32-ма процесорами накладні витрати були неприйнятно великими.
% 
% Сучасні системи SMP архітектури звичайно складаються із декількох однорідних мікропроцесорів, що серійно випускаються, і масиву спільної пам'яті, підключення до якої виконується за допомогою спільної шини або за допомогою комутатора. Наявність спільної пам'яті значно спрощує організацію взаємодії процесорів між собою і програмування, оскільки паралельна програма працює в єдиному адресному просторі.
%  
% Але за цією простотою ховаються великі проблеми, притаманні системам цього типу. Усі вони так чи інакше пов'язані з оперативною пам'яттю. Річ у тому, що зараз навіть в однопроцесорних системах недосконалим місцем є оперативна пам'ять, швидкість роботи якої значно відстала від швидкості роботи процесора. Для того, щоб згладити цей розрив, сучасні процесори забезпечуються швидкісною буферною пам'яттю (кеш-пам'яттю), швидкість роботи якої значно вища, ніж швидкість роботи основної пам'яті. Очевидно, що при проектуванні багатопроцесорних систем ці питання потребують ще більш детального вивчення.
% 
% Окрім добре відомої проблеми конфліктів при зверненні до спільної шини пам'яті постала і нова задача, пов'язана з ієрархічною структурою організації пам'яті сучасних комп'ютерів. 
% 
% У багатопроцесорних системах, побудованих на базі мікропроцесорів із вбудованою кеш-пам'яттю, порушується принцип рівноправного доступу до будь-якої точки пам'яті. Дані, що знаходяться в кеш-пам'яті деякого процесора, недоступні для інших процесорів. Це означає, що після кожної модифікації копії деякої змінної, що знаходиться в кеш-пам'яті якого-небудь процесора, необхідно виконувати синхронну модифікацію цієї змінної, розташованої в основній пам'яті.
% 
% З більшим або меншим успіхом ці проблеми вирішуються в рамках загальноприйнятої в даний час архітектури ccNUMA. В цій архітектурі пам'ять фізично розподілена, але логічно загальнодоступна. Це, з одного боку, дозволяє працювати з єдиним адресним простором, а, з іншого, збільшує масштабованість систем.  Когерентність кеш-пам'яті підтримується на апаратному рівні, що, проте, не звільняє від накладних витрат на її підтримку. 
% 
% На відміну від класичних систем SMP пам'ять стає трирівневою:
% 
% \begin{itemize}
% \item кеш-пам'ять процесора;
% \item локальна оперативна пам'ять;
% \item віддалена оперативна пам'ять;
% \end{itemize}
% 
% Час звернення до різних рівнів може відрізнятися на порядок, що сильно ускладнює написання ефективних програм для таких систем. Перераховані обставини значно обмежують можливості з нарощування продуктивності ccNUMA систем шляхом простого збільшення кількості процесорів.
% 
% Неприємною властивістю SMP систем є те, що їх вартість зростає швидше, ніж продуктивність при збільшенні кількості процесорів в системі. Крім того, через затримки під час звернення до загальної пам'яті неминуче взаємне гальмування при паралельному виконанні навіть незалежних програм.

\subsection{Системи з масовим паралелізмом (МРР)}


Проблеми, притаманні мультипроцесорним системам зі спільною пам'яттю, природним чином усуваються в системах з масовим паралелізмом. Комп'ютери цього типу є багатопроцесорними системами з розподіленою пам'яттю, в яких, за допомогою деякого комунікаційного середовища, об'єднуються однорідні обчислювальні вузли.
  
Кожен з вузлів складається з одного або кількох процесорів, власної оперативної пам'яті, комунікаційного обладнання, підсистеми введення/виведення, тобто має все необхідне для незалежного функціонування. 

При цьому на кожному вузлі може функціонувати або повноцінна операційна система (як в системі RS/6000 SP2), або урізаний варіант, що підтримує лише базові функції ядра, а повноцінна ОС працює на спеціальному керівному комп'ютері (як в системах Cray T3E, nCUBE2). 

Процесори в таких системах мають прямий доступ лише до своєї локальної пам'яті. Доступ до пам'яті інших вузлів реалізується, зазвичай, за допомогою механізму передачі повідомлень. Така архітектура обчислювальної системи усуває одночасно як проблему конфліктів при зверненні до пам'яті, так і проблему когерентності кеш-пам'яті. 

Це дає можливість практично необмеженого нарощування кількості процесорів в системі, збільшуючи тим самим її продуктивність. Успішно функціонують MPP з сотням і тисячами процесорів (ASCI White – 8192, Blue Mountain – 6144). 

Важливою властивістю MPP систем є їх висока міра масштабованості. Залежно від обчислювальних потреб для досягнення необхідної продуктивності потрібно просто зібрати систему з потрібною кількістю вузлів.

На практиці все, звичайно, набагато складніше. Усунення одних ускладнень, як це зазвичай буває, породжує інші. Для MPP систем насамперед виникає питання ефективності комунікаційного середовища. Різні виробники MPP систем використовували різні топології. У комп'ютерах Intel Paragon процесори утворювали прямокутну двовимірну сітку. Для цього в кожному вузлі досить чотирьох комунікаційних каналів. У комп'ютерах Cray T3D/T3E використовувалася топологія тривимірного тора. Відповідно, у вузлах цього комп'ютера було шість комунікаційних каналів. Фірма nCUBE використовувала в своїх комп'ютерах топологію n-вимірного гіперкуба. 

Кожна з розглянутих топологій має свої переваги і недоліки. Зазначимо, що при обміні даними між процесорами, які не є найближчими сусідами, відбувається трансляція даних через проміжні вузли. Очевидно, що у вузлах мають бути передбачені якісь апаратні засоби, які звільняли б центральний процесор від участі в трансляції даних. 

Останнім часом для з'єднання обчислювальних вузлів частіше використовується ієрархічна система високошвидкісних комутаторів, як це вперше було реалізовано в комп'ютерах IBM SP2. Така топологія дає можливість прямого обміну даними між будь-якими вузлами, без участі в цьому проміжних вузлів.

Системи з розподіленою пам'яттю ідеально підходять для пареллельного виконання незалежних програм, оскільки при цьому кожна програма виконується на своєму вузлі і жодним чином не впливає на виконання інших програм. Проте при розробці паралельних програм доводиться враховувати складнішу, ніж в SMP системах, організацію пам'яті. Оперативна пам'ять в MPP системах має 3-х рівневу структуру:

\begin{itemize}
\item кеш-пам'ять процесорів;
\item локальна оперативна пам'ять;
\item оперативна пам'ять інших вузлів.
\end{itemize}

При цьому відсутня можливість прямого доступу до даних, розташованих в інших вузлах. Для їх використання ці дані мають бути заздалегідь передані в той вузол, який в даний момент їх потребує. Це значно ускладнює програмування. Крім того, обмін даними між вузлами виконується значно повільніше, ніж обробка даних в локальній оперативній пам'яті вузлів. Тому написання ефективних паралельних програм для таких комп'ютерів є складнішим завданням, ніж для SMP систем.

\subsection{Кластерні системи}

Кластерні технології стали логічним продовженням розвитку ідей, закладених в архітектурі MPP систем. 

Якщо процесорний модуль в MPP системі є закінченою обчислювальною системою, то наступний крок напрошується сам собою: чому б в якості обчислювальних вузлів не використовувати звичайні комп'ютери, що серійно випускаються~\cite{beowulf}.  

Розвиток комунікаційних технологій, а саме, поява високошвидкісного мережевого обладнання і спеціального програмного забезпечення, такого як система MPI, що реалізовує механізм передачі повідомлень над стандартними мережевими протоколами, зробили кластерні технології загальнодоступними.  

Сьогодні не складає великих труднощів створити невелику кластерну систему, об'єднавши обчислювальні потужності комп'ютерів окремої лабораторії або учбового класу. Привабливою рисою кластерних технологій є те, що вони дозволяють для досягнення необхідної продуктивності, об'єднувати в єдині обчислювальні системи комп'ютери різного типу, починаючи від персональних комп'ютерів і закінчуючи потужними суперкомп'ютерами.

Широкого поширення кластерні технології набули як засіб створення систем суперкомп'ютерного класу зі складових частин масового виробництва, що значно знижує вартість обчислювальної системи. Звичайно, про повну еквівалентність цих систем говорити не доводиться. Як вказувалося в попередньому розділі, продуктивність системи з розподіленою пам'яттю дуже сильно залежить від продуктивності комунікаційного середовища.
 
Комунікаційне середовище можна досить повно охарактеризувати двома параметрами: латентністю -- часом затримки при відсиланні повідомлення та пропускною здатністю -- швидкістю передачі інформації. Так для комп'ютера Cray T3D ці параметри складають відповідно 1 мкс і 480 мб/с, а для кластера, у якому в якості комунікаційного середовища використана мережа Fast Ethernet, -- 100 мкс і 10 мб/с відповідно. Це частково пояснює дуже високу вартість суперкомп'ютера. При таких параметрах, як у розглянутого кластера, знайдеться не так багато задач, які можна ефективно вирішувати на досить великій кількості процесорів.

Коротко кажучи, кластер -- це зв'язаний набір повноцінних комп'ютерів, що використовується як єдиний обчислювальний ресурс. Переваги кластерної системи перед набором незалежних комп'ютерів очевидні.

По-перше, розроблено безліч диспетчерських систем пакетної обробки завдань, що дозволяють відправити завдання на обробку кластера вцілому, а не якомусь окремому комп'ютеру. Ці диспетчерські системи автоматично розподіляють завдання за вільними обчислювальними вузлами або буферизують їх за відсутності таких, що дозволяє забезпечити більш рівномірне і ефективне завантаження комп'ютерів. По-друге, з'являється можливість спільного використання обчислювальних ресурсів декількох комп'ютерів для вирішення одного завдання. Для створення кластерів, зазвичай, використовуються або прості однопроцесорні персональні комп'ютери, або двох- чи чотирипроцесорні SMP-сервери ~\cite{supercomp-architecture-proceed,supercomp-architecture-art}.

При цьому не накладається жодних обмежень на склад і архітектуру вузлів. Кожен з вузлів може функціонувати під управлінням своєї власної операційної системи. Найчастіше використовуються стандартні ОС: Linux, FreeBSD, Solaris, Tru64 Unix, Windows. 

У тих випадках, коли вузли кластера неоднорідні, то говорять про гетерогенні кластери ~\cite{getero-art,getero-taganrog-art}.

При створенні кластера можна виділити два підходи. Перший підхід застосовується при створенні невеликої кластерної системи. У кластер об'єднуються повнофункціональні комп'ютери, які продовжують працювати і як самостійна одиниця, наприклад, комп'ютери учбового класу або робочі станції лабораторії, як у навчальній системі UACLUSTER~\cite{cluster-uacluster}. 

Другий підхід застосовується в тих випадках, коли цілеспрямовано створюється потужний обчислювальний ресурс. Тоді системні блоки комп'ютерів компактно розміщуються в спеціальний стійках, а для управління системою і для запуску задач виділяється один або декілька повнофункціональних комп'ютерів, так званих хост-комп’ютерів. У цьому випадку немає необхідності забезпечувати комп'ютери обчислювальних вузлів графічними картами, моніторами, дисковими накопичувачами та іншим периферійним обладнанням, що значно знижує вартість системи.

\subsection{Архітектура типу сузір'я}

Сузір'я (Constellation) -- це кластер великих SMP вузлів, в якому кількість процесорів на вузол більше, ніж кількість вузлів.

Сьогодні для кластера стандарним є вузол з 16 ядрами. У системах типу сузір'я використовуються вузли із значно більшою кількістю ядер, SMP-системи з 100--1000 ядрами. 

Також, згідно визначення, до архітектури сузір'я відносяться гібридні вузли з графічними процесорами, оскільки кожен такий вузол містить кілька сотень процесорних елементів.

Загальна пам'ять, доступна всім процесам задачі у великому SMP-вузлі, дозволяє ефективно вирішувати тісно пов'язані задачі з великою кількістю обмінів даними між процесами. Це не під силу звичайним кластерам, на яких такі завдання виконуються вкрай неефективно.

Прикладами задач, для яких кластери поступаються архітектурі Constellation, є прогнозування погоди, моделювання клімату, розрахунок щільності, задачі теплопереноса.